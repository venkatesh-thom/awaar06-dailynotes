


Manual Scaling and Managing the container is hard.. 
If any container is down, manually we need to run again..
Easy for lab/home workloads, but not suggestable to run production grade workload without Orchestators.. 

Orchestrator: Auto Scaling, Self healing, Network setup, workload schedules..
K8s

K8s mainly contains 2 Parts:

Control Plane Components : Brain of K8s..

--> kube-apiserver / API Server : front door of the k8s.. (we use "kubectl" to communicate with api server )

--> etcd (Database) : It contains all the cluster data.. Information stores in "Key and Value" (name=webserver)
	Stores "Desired state" and "current state" of our k8s
	
--> Controller manager (kube-controller-manager) (Supervisor) : monitors "etcd" and helps to match the desired 	state..

--> Scheduler (kube-scheduler) (Decision maker) : This decides at what "node" the workload should run/create.

--> Cloud Controller manager (CCM) : helps to connect cloud provider's K8s cluster i.e; eks, aks, gke.. Loadbalancer.. Monitoring.. 

----

Worker Node Components : This is where our workload runs.. (ec2, fargate, server)

--> kubelet : THis is an agent runs in each worker node.. This component talks to API Server.. If pod Crashes, kubelet restarts it automatically.. 

--> Container Runtime : responsible to pull and run containers.. K8s uses "CRI-O" and "containerd".. Used to support Docker.
kubelet asks the container runtime to pull the image based on the instructions it received from api-server. This runtime starts the container inside the pod.

--> kube-proxy (network) : This componet manages the networking rules and pod communications.. This is responsible to allocate a unique IP address for every pod we run.. 

---

pod : Smallest deployable unit in K8s.. Pod is nothing but a wrapper to the container.. 

Replica Set : 

Deployment : 

----

EKS : Elastic Kubernetes Service : Fully managed K8s service provided by AWS. 

Managed Control plane by AWS. AWS Provides HA and reliability to eks cluster as default option.
We can integrate with other AWS services. (IRSA/VPC/ALB/ route53 / Cloudwatch)
for HA, AWS maintains 3 copies of control plane components across 3 AZs.. 
We dont get access to the control plane nodes managed by AWS.

EKS Data Plane operation : Its the place where our workload runs.. 

EC2 Self Managed Node group : 
--> As a customer, we have to provision the ec2 instances, we have to configure the ec2 instances..!!
--> We have to take care about the OS updates/ patches, Scaling activities.

EC2 Managed Node group : 
--> AWS automatically provision and managed the ec2 instances.
--> AWS take care about the OS updates/ patches, Scaling activities.
--> AWS uses Amazon Linux 2023 / Bottlerocket.
--> We need to choose the required number of instances and instance type.

AWS Auto-Mode : EKS Auto Mode extends AWS management of Kubernetes clusters beyond the cluster itself, to allow AWS to also set up and manage the infrastructure that enables the smooth operation of your workloads. You can delegate key infrastructure decisions and leverage the expertise of AWS for day-to-day operations. Cluster infrastructure managed by AWS includes many Kubernetes capabilities as core components, as opposed to add-ons, such as compute autoscaling, pod and service networking, application load balancing, cluster DNS, block storage, and GPU support.

AWS fargate : Serverless, No nodes to manage, Directly we can deploy pods. pay-as-you-go.. 


===========

Most of the times, we use eksctl to create EKS cluster.. THis has some pre-req:

1. aws cli tools install & configure		(aws.amazon.com/cli)
2. kubectl									(https://kubernetes.io/docs/tasks/tools/)

3. install eksctl
https://docs.aws.amazon.com/eks/latest/eksctl/installation.html


eksctl create cluster --name=ekswithavinash --version 1.34 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type c7i-flex.large --nodes 2 --node-ami-family=AmazonLinux2023 --managed



eksctl create cluster --name=ekswithavinash-uat --version 1.34 --region ap-south-1 --zones=ap-south-1a,ap-south-1b --nodegroup-name ng-default --node-type c7i-flex.large --nodes 2 --node-ami-family=AmazonLinux2023 --managed


----

CNI : Container Network Interface		: THis is a network plugin.. This helps to extend the container network with existing network..

VPC CNI : Virtual private Cloud Container Network interface : 

--> IPAM (IP Address management)
--> pod Networking
--> k8s network polocies

-----

OIDC : IAM OpenID Connect : This allows AWS IAM to authenticate with K8s Service accounts, IAM Permissions / roles.

aws eks describe-cluster --name ekswithavinash

eksctl utils associate-iam-oidc-provider --region ap-south-1 --cluster ekswithavinash --approve

EKS CLuster configuratio will be stored in our local laptop.. 
Path : /Users/username/.kube/config


aws eks update-kubeconfig --region ap-south-1 --name ekswithavinash

---

eksctl 		--> helps to create cluster
kubectl 	--> helps to interact with k8s cluster

kubectl get pods						--> Display pods from default namespace

kubectl get pods -A						--> Display pods from ALL namespace
kubectl get pods --all-namespaces		


kubectl apply -f 01-pod.yaml			--> THis will create a pod
kubectl apply -f 01-pod.yaml --dry-run=server	--> This will dry run (identify the potential issues before deploying)


kubectl describe pods <pod-name>		


eksctl delete cluster --name ekswithavinash			--> Delete your cluster

kubectl delete pods <pod-name> <pod-name-2>			--> Deletes the pods


kubectl logs <pod-name>								--> Displays the logs
kubectl logs -f <pod-name>							--> Displays the live logs from pod

kubectl describe pods <pod-name>					--> Displays the pods info inclusing the events



Servcie : helps to expose the pods (internally / externally)


kubectl get services
kubectl get svc


ClusterIP : We cannot expose it to outside world. THis is default svc, if we dont mention service type in manifest file.

NodePort : Expose our workload outside using node/ec2-instance public IP.















